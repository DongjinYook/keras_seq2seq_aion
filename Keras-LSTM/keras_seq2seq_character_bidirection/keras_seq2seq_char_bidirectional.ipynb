{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence-to-sequence learning in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training\n",
    "epochs = 10  # Number of epochs to train for\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "input_data_path = 'data/aion_train_v.txt'\n",
    "target_data_path = \"data/aion_train_t.txt\"\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(input_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = f.readlines()\n",
    "    for i in reader:\n",
    "        input_texts.append(i)\n",
    "        for char in i:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "            \n",
    "with open(target_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = f.readlines()\n",
    "    for i in reader:\n",
    "        # '\\t'  = start sequence\n",
    "        # '\\n' = end sequence\n",
    "        target_text = '\\t' + i\n",
    "        target_texts.append(target_text)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters)) ## character를 순서대로 정리 ㄱ~ㅎ\n",
    "target_characters = sorted(list(target_characters))  ## character를 순서대로 정리 a~z ㄱ~ㅎ\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])  ## input texts에 있는 문장 길이를 순서대로 다른 리스트에 등록하고 최대 찾기\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '가', '각', '갈', '감', '강', '거', '검', '겠', '견', '결', '계', '고', '곰', '관', '광', '귄', '그', '근', '금', '기', '깡', '끼', '나', '냥', '네', '넬', '능', '니', '다', '단', '달', '대', '더', '데', '도', '동', '둠', '드', '디', '라', '랄', '랑', '랙', '량', '레', '렌', '려', '력', '로', '룡', '루', '룬', '룽', '르', '를', '리', '린', '마', '막', '만', '망', '맹', '메', '명', '모', '몽', '무', '묵', '민', '바', '반', '백', '벌', '베', '벨', '변', '보', '복', '불', '브', '블', '비', '빅', '빛', '빨', '사', '상', '새', '생', '샤', '서', '설', '성', '세', '셀', '수', '쉬', '슈', '스', '승', '시', '신', '실', '심', '십', '아', '악', '안', '야', '양', '어', '에', '여', '역', '영', '예', '옐', '오', '와', '요', '용', '우', '웨', '유', '으', '은', '을', '의', '이', '인', '일', '자', '작', '적', '전', '점', '정', '제', '좀', '주', '줘', '지', '진', '집', '징', '창', '채', '챈', '천', '체', '초', '최', '추', '축', '치', '침', '카', '칸', '켈', '쿠', '퀘', '큐', '크', '키', '타', '탑', '테', '토', '톤', '통', '투', '트', '티', '팅', '파', '팬', '퍼', '펭', '포', '표', '프', '플', '픽', '핑', '하', '한', '합', '해', '험', '헤', '혁', '현', '혜', '호', '황']\n"
     ]
    }
   ],
   "source": [
    "print(input_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '(', ')', '.', 'a', 'b', 'c', 'e', 'f', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', '가', '각', '갈', '감', '강', '검', '견', '결', '계', '고', '곰', '관', '광', '귄', '그', '근', '금', '기', '깡', '끼', '나', '냥', '네', '넬', '능', '니', '다', '단', '달', '대', '더', '데', '도', '동', '둠', '드', '디', '라', '랄', '랑', '랙', '량', '레', '렌', '력', '로', '룡', '루', '룬', '룽', '르', '리', '린', '마', '막', '만', '망', '맹', '메', '명', '모', '몽', '묵', '민', '바', '반', '백', '벌', '베', '벨', '보', '복', '불', '브', '블', '비', '빛', '사', '상', '새', '생', '샤', '성', '세', '셀', '수', '쉬', '슈', '스', '승', '시', '신', '실', '심', '아', '악', '안', '야', '양', '어', '에', '여', '역', '영', '예', '옐', '오', '와', '요', '용', '우', '웨', '유', '은', '을', '의', '이', '인', '일', '작', '적', '전', '정', '제', '지', '진', '집', '징', '채', '챈', '천', '체', '초', '최', '추', '축', '치', '침', '카', '칸', '켈', '쿠', '크', '키', '타', '탑', '테', '토', '톤', '통', '투', '트', '티', '파', '팬', '퍼', '펭', '포', '표', '프', '플', '픽', '핑', '하', '한', '해', '험', '헤', '혁', '현', '혜', '호', '황']\n"
     ]
    }
   ],
   "source": [
    "print(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 86644\n",
      "Number of unique input characters: 197\n",
      "Number of unique output characters: 196\n",
      "Max sequence length for inputs: 30\n",
      "Max sequence length for outputs: 23\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input characters:', num_encoder_tokens)\n",
    "print('Number of unique output characters:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hot vector 생성 - [0,1,...0,...0,0]\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens)) ##[문장길이, one-hot vector길이]\n",
    "encoder = Bidirectional(LSTM(latent_dim, return_state=True),merge_mode=\"sum\") ## latent_dim == size\n",
    "# encoder = LSTM(latent_dim, return_state=True)## latent_dim == size\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True) ## return_sequences -> output이 다음 state의 input으로 들어감\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장시 Warning 발생하지만 무시. // keras 버그 인듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69315 samples, validate on 17329 samples\n",
      "Epoch 1/10\n",
      "69315/69315 [==============================] - 77s 1ms/step - loss: 0.3852 - val_loss: 3.8751e-04\n",
      "Epoch 2/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 9.9262e-04 - val_loss: 3.9154e-07\n",
      "Epoch 3/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 2.1149e-07 - val_loss: 1.4968e-07\n",
      "Epoch 4/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 1.2979e-07 - val_loss: 1.1716e-07\n",
      "Epoch 5/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 1.0998e-07 - val_loss: 1.0508e-07\n",
      "Epoch 6/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 1.0155e-07 - val_loss: 9.9216e-08\n",
      "Epoch 7/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 9.7217e-08 - val_loss: 9.5985e-08\n",
      "Epoch 8/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 9.4604e-08 - val_loss: 9.3697e-08\n",
      "Epoch 9/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 9.2798e-08 - val_loss: 9.2349e-08\n",
      "Epoch 10/10\n",
      "69315/69315 [==============================] - 76s 1ms/step - loss: 9.1734e-08 - val_loss: 9.1484e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/anaconda3/envs/my_1.8/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_3/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_4/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('model_save/aion_slu_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "#decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "#decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                     [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == \"\\n\" or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 감정 표현 동작 하자.\n",
      "\n",
      "Decoded sentence: motion(감정 표현).\n",
      "\n",
      "-\n",
      "Input sentence: 에레슈키갈의 사도 변신을 시작 해줘.\n",
      "\n",
      "Decoded sentence: transform(에레슈키갈의 사도).\n",
      "\n",
      "-\n",
      "Input sentence: 정신 큐빅을 시작 하소서.\n",
      "\n",
      "Decoded sentence: cubic(정신).\n",
      "\n",
      "-\n",
      "Input sentence: 황금 깡통 변신을 하게나.\n",
      "\n",
      "Decoded sentence: transform(황금 깡통).\n",
      "\n",
      "-\n",
      "Input sentence: 최근 퀘스트 빨리 하지.\n",
      "\n",
      "Decoded sentence: quest(최근).\n",
      "\n",
      "-\n",
      "Input sentence: 카이시넬 변신을 빨리 합시다.\n",
      "\n",
      "Decoded sentence: transform(카이시넬).\n",
      "\n",
      "-\n",
      "Input sentence: 감정 표현 동작 하여라.\n",
      "\n",
      "Decoded sentence: motion(감정 표현).\n",
      "\n",
      "-\n",
      "Input sentence: 루나 파견대 변신을 빨리 하게.\n",
      "\n",
      "Decoded sentence: transform(루나 파견대).\n",
      "\n",
      "-\n",
      "Input sentence: 광대 변신을 빨리 하여라.\n",
      "\n",
      "Decoded sentence: transform(광대).\n",
      "\n",
      "-\n",
      "Input sentence: 징표 동작 지금 좀.\n",
      "\n",
      "Decoded sentence: motion(징표).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 레파르 혁명단 변신 시작 하렴.\n",
      "\n",
      "Decoded sentence: transform(레파르 혁명단).\n",
      "\n",
      "-\n",
      "Input sentence: 레드 펭귄 변신을 지금 하십시오.\n",
      "\n",
      "Decoded sentence: transform(레드 펭귄).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input_texts = []\n",
    "\n",
    "with open(\"data/aion_test_v.txt\", \"r\", encoding='utf=8') as f:\n",
    "    reader =f.readlines()\n",
    "    for i in reader:\n",
    "        test_input_texts.append(i)\n",
    "\n",
    "test_encoder_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for i, test_input_text in enumerate(test_input_texts):\n",
    "    for t, char in enumerate(test_input_text):\n",
    "        test_encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "for seq_index in range(28, 30):\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "  \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "  words = []\n",
    "  #for space_separated_fragment in sentence.strip().split():\n",
    "  words.extend(_WORD_SPLIT.split(sentence))\n",
    "  return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자열을 입력하세요: 지금 백곰 변신 시작 해줘\n",
      "------------------------------------\n",
      "Decoded sentence: transform(백곰).\n",
      "\n",
      "revised Decoded sentence: transform(백곰).\n",
      "\n",
      "문자열을 입력하세요: 어서 투명으로 지도 설정 하겠니\n",
      "------------------------------------\n",
      "Decoded sentence: map(투명).\n",
      "\n",
      "revised Decoded sentence: map(투명).\n",
      "\n",
      "문자열을 입력하세요: 에레슈키갈 드라칸 어서 변신 하겠니\n",
      "------------------------------------\n",
      "Decoded sentence: transform(에레슈키갈 드라칸).\n",
      "\n",
      "revised Decoded sentence: transform(에레슈키갈 드라칸).\n",
      "\n",
      "문자열을 입력하세요: 검은갈기 라이칸 빨리 변신 하겠니\n",
      "------------------------------------\n",
      "Decoded sentence: transform(검은갈기 라이칸).\n",
      "\n",
      "revised Decoded sentence: transform(검은갈기 라이칸).\n",
      "\n",
      "문자열을 입력하세요: 유프로시네 마을에 거점 이동을 시작 하시오\n",
      "------------------------------------\n",
      "Decoded sentence: teleport(유프로시네 마을).\n",
      "\n",
      "revised Decoded sentence: teleport(유프로시네 마을).\n",
      "\n",
      "문자열을 입력하세요: 세네모네아의 야영지 거점 이동 어서 시작 하겠니\n",
      "------------------------------------\n",
      "Decoded sentence: teleport(세네모네아의 야영지).\n",
      "\n",
      "revised Decoded sentence: teleport(세네모네아의 야영지).\n",
      "\n",
      "문자열을 입력하세요: 야영지 세네모네아의 거점 이동 어서 시작 하겠니\n",
      "------------------------------------\n",
      "Decoded sentence: teleport(세네모네아의 야영지).\n",
      "\n",
      "revised Decoded sentence: teleport(세네모네아의 야영지).\n",
      "\n",
      "문자열을 입력하세요: 마을에 유프로시네 거점 이동을 시작 하시오\n",
      "------------------------------------\n",
      "Decoded sentence: teleport(유프로시네 마을).\n",
      "\n",
      "revised Decoded sentence: teleport(유프로시네 마을).\n",
      "\n",
      "문자열을 입력하세요: 슈퍼맨으로 변신 하시오\n",
      "------------------------------------\n",
      "Decoded sentence: transform(슈고).\n",
      "\n",
      "revised Decoded sentence: transform().\n",
      "\n",
      "문자열을 입력하세요: 배트맨으로 변신 하시오\n",
      "------------------------------------\n",
      "Decoded sentence: transform(웨다).\n",
      "\n",
      "revised Decoded sentence: transform().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input_texts = []\n",
    "char_input_tests= set()\n",
    "\n",
    "with open(\"data/aion_test_v.txt\", \"r\", encoding='utf=8') as f:\n",
    "    reader =f.readlines()\n",
    "    for i in reader:\n",
    "        test_input_texts.append(i)\n",
    "        \n",
    "test_encoder_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for i in range(10):\n",
    "    char_input_tests= set()\n",
    "    \n",
    "    s = input('문자열을 입력하세요: ')\n",
    "    s = s.strip()\n",
    "    for char in s:\n",
    "        if char not in char_input_tests:\n",
    "            char_input_tests.add(char)\n",
    "            \n",
    "    for t, char in enumerate(s):\n",
    "        if char in input_characters:\n",
    "            test_encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        else:\n",
    "            char= \" \"\n",
    "            test_encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    char_input_tests.add(\" \")\n",
    "    input_seq = test_encoder_input_data[i: i+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('------------------------------------')\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    revised_decoded_sentence = basic_tokenizer(decoded_sentence)\n",
    "    for t, char in enumerate(revised_decoded_sentence[2]):\n",
    "        if char not in char_input_tests:\n",
    "            revised_decoded_sentence[2]=\"\"\n",
    "    revised_decoded_sentence = \"\".join(revised_decoded_sentence)\n",
    "    print('revised Decoded sentence:', revised_decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_1.8]",
   "language": "python",
   "name": "conda-env-my_1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
