{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence-to-sequence learning in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training\n",
    "epochs = 100  # Number of epochs to train for\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "input_data_path = 'data/aion_train_v.txt'\n",
    "target_data_path = \"data/aion_train_t.txt\"\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(input_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = f.readlines()\n",
    "    for i in reader:\n",
    "        input_texts.append(i)\n",
    "        for char in i:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "            \n",
    "with open(target_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = f.readlines()\n",
    "    for i in reader:\n",
    "        # '\\t'  = start sequence\n",
    "        # '\\n' = end sequence\n",
    "        target_text = '\\t' + i\n",
    "        target_texts.append(target_text)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters)) ## character를 순서대로 정리 ㄱ~ㅎ\n",
    "target_characters = sorted(list(target_characters))  ## character를 순서대로 정리 a~z ㄱ~ㅎ\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])  ## input texts에 있는 문장 길이를 순서대로 다른 리스트에 등록하고 최대 찾기\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '.', '가', '각', '갈', '감', '강', '거', '검', '게', '겠', '견', '결', '계', '고', '곰', '관', '광', '귄', '그', '근', '금', '기', '까', '깡', '끼', '나', '냥', '네', '넬', '능', '니', '다', '단', '달', '대', '더', '데', '도', '동', '둠', '드', '디', '라', '랄', '랑', '랙', '량', '레', '렌', '려', '력', '렴', '로', '룡', '루', '룬', '룽', '르', '를', '리', '린', '마', '막', '만', '망', '맹', '메', '명', '모', '몽', '무', '묵', '민', '바', '반', '백', '벌', '베', '벨', '변', '보', '복', '불', '브', '블', '비', '빅', '빛', '빨', '사', '상', '새', '생', '샤', '서', '설', '성', '세', '셀', '소', '수', '쉬', '슈', '스', '승', '시', '신', '실', '심', '십', '아', '악', '안', '야', '양', '어', '에', '여', '역', '영', '예', '옐', '오', '와', '요', '용', '우', '웨', '유', '은', '을', '의', '이', '인', '일', '자', '작', '적', '전', '점', '정', '제', '좀', '주', '줘', '지', '진', '집', '징', '채', '챈', '천', '체', '초', '최', '추', '축', '치', '침', '카', '칸', '켈', '쿠', '퀘', '큐', '크', '키', '타', '탑', '테', '토', '톤', '통', '투', '트', '티', '팅', '파', '팬', '퍼', '펭', '포', '표', '프', '플', '픽', '핑', '하', '한', '합', '해', '험', '헤', '혁', '현', '혜', '호', '황']\n"
     ]
    }
   ],
   "source": [
    "print(input_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '(', ')', '.', 'a', 'b', 'c', 'e', 'f', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', '가', '각', '갈', '감', '강', '검', '견', '결', '계', '고', '곰', '관', '광', '귄', '그', '근', '금', '기', '깡', '끼', '나', '냥', '네', '넬', '능', '니', '다', '단', '달', '대', '더', '데', '도', '동', '둠', '드', '디', '라', '랄', '랑', '랙', '량', '레', '렌', '력', '로', '룡', '루', '룬', '룽', '르', '리', '린', '마', '막', '만', '망', '맹', '메', '명', '모', '몽', '묵', '민', '바', '반', '백', '벌', '베', '벨', '보', '복', '불', '브', '블', '비', '빛', '사', '상', '새', '생', '샤', '성', '세', '셀', '수', '쉬', '슈', '스', '승', '시', '신', '실', '심', '아', '악', '안', '야', '양', '어', '에', '여', '역', '영', '예', '옐', '오', '와', '요', '용', '우', '웨', '유', '은', '을', '의', '이', '인', '일', '작', '적', '전', '정', '제', '지', '진', '집', '징', '채', '챈', '천', '체', '초', '최', '추', '축', '치', '침', '카', '칸', '켈', '쿠', '크', '키', '타', '탑', '테', '토', '톤', '통', '투', '트', '티', '파', '팬', '퍼', '펭', '포', '표', '프', '플', '픽', '핑', '하', '한', '해', '험', '헤', '혁', '현', '혜', '호', '황']\n"
     ]
    }
   ],
   "source": [
    "print(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 15182\n",
      "Number of unique input characters: 200\n",
      "Number of unique output characters: 196\n",
      "Max sequence length for inputs: 27\n",
      "Max sequence length for outputs: 23\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input characters:', num_encoder_tokens)\n",
    "print('Number of unique output characters:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hot vector 생성 - [0,1,...0,...0,0]\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens)) ##[문장길이, one-hot vector길이]\n",
    "encoder = Bidirectional(LSTM(latent_dim, return_state=True),merge_mode=\"sum\") ## latent_dim == size\n",
    "# encoder = LSTM(latent_dim, return_state=True)## latent_dim == size\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True) ## return_sequences -> output이 다음 state의 input으로 들어감\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장시 Warning 발생하지만 무시. // keras 버그 인듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12145 samples, validate on 3037 samples\n",
      "Epoch 1/100\n",
      "12145/12145 [==============================] - 14s 1ms/step - loss: 1.3905 - val_loss: 0.7254\n",
      "Epoch 2/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.4110 - val_loss: 0.1840\n",
      "Epoch 3/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.1207 - val_loss: 0.0636\n",
      "Epoch 4/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.0583 - val_loss: 0.0032\n",
      "Epoch 5/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.0067 - val_loss: 1.4646e-04\n",
      "Epoch 6/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.0076 - val_loss: 7.5323e-05\n",
      "Epoch 7/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.0029 - val_loss: 8.9756e-06\n",
      "Epoch 8/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.0033 - val_loss: 2.8671e-06\n",
      "Epoch 9/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.0029 - val_loss: 1.7146e-06\n",
      "Epoch 10/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 0.0020 - val_loss: 3.2793e-06\n",
      "Epoch 11/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.0038e-06 - val_loss: 4.7882e-07\n",
      "Epoch 12/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 3.8487e-07 - val_loss: 3.2152e-07\n",
      "Epoch 13/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 2.8195e-07 - val_loss: 2.5256e-07\n",
      "Epoch 14/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 2.3025e-07 - val_loss: 2.1286e-07\n",
      "Epoch 15/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.9818e-07 - val_loss: 1.8697e-07\n",
      "Epoch 16/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.7609e-07 - val_loss: 1.6804e-07\n",
      "Epoch 17/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.6021e-07 - val_loss: 1.5434e-07\n",
      "Epoch 18/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.4809e-07 - val_loss: 1.4352e-07\n",
      "Epoch 19/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.3868e-07 - val_loss: 1.3539e-07\n",
      "Epoch 20/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.3123e-07 - val_loss: 1.2864e-07\n",
      "Epoch 21/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.2506e-07 - val_loss: 1.2278e-07\n",
      "Epoch 22/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.1993e-07 - val_loss: 1.1814e-07\n",
      "Epoch 23/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.1552e-07 - val_loss: 1.1391e-07\n",
      "Epoch 24/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.1162e-07 - val_loss: 1.1031e-07\n",
      "Epoch 25/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.0837e-07 - val_loss: 1.0739e-07\n",
      "Epoch 26/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.0576e-07 - val_loss: 1.0507e-07\n",
      "Epoch 27/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.0343e-07 - val_loss: 1.0274e-07\n",
      "Epoch 28/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 1.0131e-07 - val_loss: 1.0066e-07\n",
      "Epoch 29/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.9391e-08 - val_loss: 9.8848e-08\n",
      "Epoch 30/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.7807e-08 - val_loss: 9.7343e-08\n",
      "Epoch 31/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.6351e-08 - val_loss: 9.5976e-08\n",
      "Epoch 32/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.5061e-08 - val_loss: 9.4742e-08\n",
      "Epoch 33/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.3983e-08 - val_loss: 9.3735e-08\n",
      "Epoch 34/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.2954e-08 - val_loss: 9.2672e-08\n",
      "Epoch 35/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.1940e-08 - val_loss: 9.1682e-08\n",
      "Epoch 36/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.1114e-08 - val_loss: 9.0899e-08\n",
      "Epoch 37/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 9.0459e-08 - val_loss: 9.0279e-08\n",
      "Epoch 38/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.9887e-08 - val_loss: 8.9674e-08\n",
      "Epoch 39/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.9240e-08 - val_loss: 8.9069e-08\n",
      "Epoch 40/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.8658e-08 - val_loss: 8.8580e-08\n",
      "Epoch 41/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.8159e-08 - val_loss: 8.8162e-08\n",
      "Epoch 42/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.7672e-08 - val_loss: 8.7701e-08\n",
      "Epoch 43/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.7254e-08 - val_loss: 8.7196e-08\n",
      "Epoch 44/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.6853e-08 - val_loss: 8.6789e-08\n",
      "Epoch 45/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.6494e-08 - val_loss: 8.6391e-08\n",
      "Epoch 46/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.6143e-08 - val_loss: 8.6058e-08\n",
      "Epoch 47/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.5830e-08 - val_loss: 8.5786e-08\n",
      "Epoch 48/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.5560e-08 - val_loss: 8.5483e-08\n",
      "Epoch 49/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.5290e-08 - val_loss: 8.5262e-08\n",
      "Epoch 50/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.5118e-08 - val_loss: 8.5105e-08\n",
      "Epoch 51/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4990e-08 - val_loss: 8.4980e-08\n",
      "Epoch 52/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4870e-08 - val_loss: 8.4848e-08\n",
      "Epoch 53/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4742e-08 - val_loss: 8.4702e-08\n",
      "Epoch 54/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4603e-08 - val_loss: 8.4534e-08\n",
      "Epoch 55/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4451e-08 - val_loss: 8.4387e-08\n",
      "Epoch 56/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4309e-08 - val_loss: 8.4231e-08\n",
      "Epoch 57/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4185e-08 - val_loss: 8.4089e-08\n",
      "Epoch 58/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.4066e-08 - val_loss: 8.3980e-08\n",
      "Epoch 59/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3935e-08 - val_loss: 8.3849e-08\n",
      "Epoch 60/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3820e-08 - val_loss: 8.3729e-08\n",
      "Epoch 61/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3714e-08 - val_loss: 8.3679e-08\n",
      "Epoch 62/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3639e-08 - val_loss: 8.3583e-08\n",
      "Epoch 63/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3577e-08 - val_loss: 8.3524e-08\n",
      "Epoch 64/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3524e-08 - val_loss: 8.3482e-08\n",
      "Epoch 65/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3482e-08 - val_loss: 8.3415e-08\n",
      "Epoch 66/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3410e-08 - val_loss: 8.3339e-08\n",
      "Epoch 67/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3341e-08 - val_loss: 8.3274e-08\n",
      "Epoch 68/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3259e-08 - val_loss: 8.3208e-08\n",
      "Epoch 69/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3197e-08 - val_loss: 8.3142e-08\n",
      "Epoch 70/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3126e-08 - val_loss: 8.3062e-08\n",
      "Epoch 71/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3086e-08 - val_loss: 8.3031e-08\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3072e-08 - val_loss: 8.2988e-08\n",
      "Epoch 73/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3045e-08 - val_loss: 8.2948e-08\n",
      "Epoch 74/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.3004e-08 - val_loss: 8.2903e-08\n",
      "Epoch 75/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2975e-08 - val_loss: 8.2883e-08\n",
      "Epoch 76/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2955e-08 - val_loss: 8.2863e-08\n",
      "Epoch 77/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2946e-08 - val_loss: 8.2851e-08\n",
      "Epoch 78/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2939e-08 - val_loss: 8.2842e-08\n",
      "Epoch 79/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2930e-08 - val_loss: 8.2840e-08\n",
      "Epoch 80/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2929e-08 - val_loss: 8.2839e-08\n",
      "Epoch 81/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2916e-08 - val_loss: 8.2833e-08\n",
      "Epoch 82/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2914e-08 - val_loss: 8.2833e-08\n",
      "Epoch 83/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2909e-08 - val_loss: 8.2822e-08\n",
      "Epoch 84/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2905e-08 - val_loss: 8.2817e-08\n",
      "Epoch 85/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2901e-08 - val_loss: 8.2808e-08\n",
      "Epoch 86/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2898e-08 - val_loss: 8.2804e-08\n",
      "Epoch 87/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2894e-08 - val_loss: 8.2803e-08\n",
      "Epoch 88/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2892e-08 - val_loss: 8.2796e-08\n",
      "Epoch 89/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2885e-08 - val_loss: 8.2797e-08\n",
      "Epoch 90/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2883e-08 - val_loss: 8.2788e-08\n",
      "Epoch 91/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2876e-08 - val_loss: 8.2781e-08\n",
      "Epoch 92/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2872e-08 - val_loss: 8.2789e-08\n",
      "Epoch 93/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2871e-08 - val_loss: 8.2781e-08\n",
      "Epoch 94/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2866e-08 - val_loss: 8.2777e-08\n",
      "Epoch 95/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2863e-08 - val_loss: 8.2780e-08\n",
      "Epoch 96/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2860e-08 - val_loss: 8.2775e-08\n",
      "Epoch 97/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2856e-08 - val_loss: 8.2773e-08\n",
      "Epoch 98/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2854e-08 - val_loss: 8.2764e-08\n",
      "Epoch 99/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2852e-08 - val_loss: 8.2765e-08\n",
      "Epoch 100/100\n",
      "12145/12145 [==============================] - 13s 1ms/step - loss: 8.2851e-08 - val_loss: 8.2764e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/anaconda3/envs/my_1.8/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('model_save/aion_slu_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "#decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "#decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                     [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == \"\\n\" or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 감정 표현 동작 하자.\n",
      "\n",
      "Decoded sentence: motion(감정 표현).\n",
      "\n",
      "-\n",
      "Input sentence: 에레슈키갈의 사도 변신을 시작 해줘.\n",
      "\n",
      "Decoded sentence: transform(에레슈키갈의 사도).\n",
      "\n",
      "-\n",
      "Input sentence: 정신 큐빅을 시작 하소서.\n",
      "\n",
      "Decoded sentence: cubic(정신).\n",
      "\n",
      "-\n",
      "Input sentence: 황금 깡통 변신을 하게나.\n",
      "\n",
      "Decoded sentence: transform(황금 깡통).\n",
      "\n",
      "-\n",
      "Input sentence: 최근 퀘스트 빨리 하지.\n",
      "\n",
      "Decoded sentence: quest(최근).\n",
      "\n",
      "-\n",
      "Input sentence: 카이시넬 변신을 빨리 합시다.\n",
      "\n",
      "Decoded sentence: transform(카이시넬).\n",
      "\n",
      "-\n",
      "Input sentence: 감정 표현 동작 하여라.\n",
      "\n",
      "Decoded sentence: motion(감정 표현).\n",
      "\n",
      "-\n",
      "Input sentence: 루나 파견대 변신을 빨리 하게.\n",
      "\n",
      "Decoded sentence: transform(루나 파견대).\n",
      "\n",
      "-\n",
      "Input sentence: 광대 변신을 빨리 하여라.\n",
      "\n",
      "Decoded sentence: transform(광대).\n",
      "\n",
      "-\n",
      "Input sentence: 징표 동작 지금 좀.\n",
      "\n",
      "Decoded sentence: motion(징표).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 레파르 혁명단 변신 시작 하렴.\n",
      "\n",
      "Decoded sentence: transform(레파르 혁명단).\n",
      "\n",
      "-\n",
      "Input sentence: 레드 펭귄 변신을 지금 하십시오.\n",
      "\n",
      "Decoded sentence: transform(레드 펭귄).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input_texts = []\n",
    "\n",
    "with open(\"data/aion_test_v.txt\", \"r\", encoding='utf=8') as f:\n",
    "    reader =f.readlines()\n",
    "    for i in reader:\n",
    "        test_input_texts.append(i)\n",
    "\n",
    "test_encoder_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for i, test_input_text in enumerate(test_input_texts):\n",
    "    for t, char in enumerate(test_input_text):\n",
    "        test_encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "for seq_index in range(28, 30):\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_1.8]",
   "language": "python",
   "name": "conda-env-my_1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
