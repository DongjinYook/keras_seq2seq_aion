{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence-to-sequence learning in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training\n",
    "epochs = 100  # Number of epochs to train for\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "input_data_path = 'data/aion_train_v.txt'\n",
    "target_data_path = \"data/aion_train_t.txt\"\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(input_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = f.readlines()\n",
    "    for i in reader:\n",
    "        input_texts.append(i)\n",
    "        for char in i:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "            \n",
    "with open(target_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = f.readlines()\n",
    "    for i in reader:\n",
    "        # '\\t'  = start sequence\n",
    "        # '\\n' = end sequence\n",
    "        target_text = '\\t' + i\n",
    "        target_texts.append(target_text)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters)) ## character를 순서대로 정리 ㄱ~ㅎ\n",
    "target_characters = sorted(list(target_characters))  ## character를 순서대로 정리 a~z ㄱ~ㅎ\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])  ## input texts에 있는 문장 길이를 순서대로 다른 리스트에 등록하고 최대 찾기\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '.', '가', '각', '갈', '감', '강', '거', '검', '게', '겠', '견', '결', '계', '고', '곰', '관', '광', '귄', '그', '근', '금', '기', '까', '깡', '끼', '나', '냥', '네', '넬', '능', '니', '다', '단', '달', '대', '더', '데', '도', '동', '둠', '드', '디', '라', '랄', '랑', '랙', '량', '레', '렌', '려', '력', '렴', '로', '룡', '루', '룬', '룽', '르', '를', '리', '린', '마', '막', '만', '망', '맹', '메', '명', '모', '몽', '무', '묵', '민', '바', '반', '백', '벌', '베', '벨', '변', '보', '복', '불', '브', '블', '비', '빅', '빛', '빨', '사', '상', '새', '생', '샤', '서', '설', '성', '세', '셀', '소', '수', '쉬', '슈', '스', '승', '시', '신', '실', '심', '십', '아', '악', '안', '야', '양', '어', '에', '여', '역', '영', '예', '옐', '오', '와', '요', '용', '우', '웨', '유', '은', '을', '의', '이', '인', '일', '자', '작', '적', '전', '점', '정', '제', '좀', '주', '줘', '지', '진', '집', '징', '채', '챈', '천', '체', '초', '최', '추', '축', '치', '침', '카', '칸', '켈', '쿠', '퀘', '큐', '크', '키', '타', '탑', '테', '토', '톤', '통', '투', '트', '티', '팅', '파', '팬', '퍼', '펭', '포', '표', '프', '플', '픽', '핑', '하', '한', '합', '해', '험', '헤', '혁', '현', '혜', '호', '황']\n"
     ]
    }
   ],
   "source": [
    "print(input_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '(', ')', '.', 'a', 'b', 'c', 'e', 'f', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', '가', '각', '갈', '감', '강', '검', '견', '결', '계', '고', '곰', '관', '광', '귄', '그', '근', '금', '기', '깡', '끼', '나', '냥', '네', '넬', '능', '니', '다', '단', '달', '대', '더', '데', '도', '동', '둠', '드', '디', '라', '랄', '랑', '랙', '량', '레', '렌', '력', '로', '룡', '루', '룬', '룽', '르', '리', '린', '마', '막', '만', '망', '맹', '메', '명', '모', '몽', '묵', '민', '바', '반', '백', '벌', '베', '벨', '보', '복', '불', '브', '블', '비', '빛', '사', '상', '새', '생', '샤', '성', '세', '셀', '수', '쉬', '슈', '스', '승', '시', '신', '실', '심', '아', '악', '안', '야', '양', '어', '에', '여', '역', '영', '예', '옐', '오', '와', '요', '용', '우', '웨', '유', '은', '을', '의', '이', '인', '일', '작', '적', '전', '정', '제', '지', '진', '집', '징', '채', '챈', '천', '체', '초', '최', '추', '축', '치', '침', '카', '칸', '켈', '쿠', '크', '키', '타', '탑', '테', '토', '톤', '통', '투', '트', '티', '파', '팬', '퍼', '펭', '포', '표', '프', '플', '픽', '핑', '하', '한', '해', '험', '헤', '혁', '현', '혜', '호', '황']\n"
     ]
    }
   ],
   "source": [
    "print(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 15182\n",
      "Number of unique input characters: 200\n",
      "Number of unique output characters: 196\n",
      "Max sequence length for inputs: 27\n",
      "Max sequence length for outputs: 23\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input characters:', num_encoder_tokens)\n",
    "print('Number of unique output characters:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hot vector 생성 - [0,1,...0,...0,0]\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Concatenate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e907f11523d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstate_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforward_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforward_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mencoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Concatenate' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens)) ##[문장길이, one-hot vector길이]\n",
    "encoder = Bidirectional(LSTM(latent_dim, return_state=True)) ## latent_dim == size\n",
    "# encoder = LSTM(latent_dim, return_state=True)## latent_dim == size\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True) ## return_sequences -> output이 다음 state의 input으로 들어감\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장시 Warning 발생하지만 무시. // keras 버그 인듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12145 samples, validate on 3037 samples\n",
      "Epoch 1/100\n",
      "12145/12145 [==============================] - 8s 648us/step - loss: 1.5294 - val_loss: 0.9705\n",
      "Epoch 2/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.6068 - val_loss: 0.4099\n",
      "Epoch 3/100\n",
      "12145/12145 [==============================] - 7s 556us/step - loss: 0.2556 - val_loss: 0.1672\n",
      "Epoch 4/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.1471 - val_loss: 0.1270\n",
      "Epoch 5/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.1085 - val_loss: 0.0843\n",
      "Epoch 6/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0706 - val_loss: 0.1060\n",
      "Epoch 7/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0480 - val_loss: 0.0353\n",
      "Epoch 8/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0321 - val_loss: 0.0915\n",
      "Epoch 9/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0255 - val_loss: 0.0079\n",
      "Epoch 10/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0182 - val_loss: 0.0025\n",
      "Epoch 11/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0147 - val_loss: 0.0033\n",
      "Epoch 12/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0092 - val_loss: 4.1179e-04\n",
      "Epoch 13/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0070 - val_loss: 1.5391e-04\n",
      "Epoch 14/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0096 - val_loss: 9.7340e-04\n",
      "Epoch 15/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0041 - val_loss: 4.0125e-05\n",
      "Epoch 16/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0074 - val_loss: 1.2912e-04\n",
      "Epoch 17/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0043 - val_loss: 0.1311\n",
      "Epoch 18/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0102 - val_loss: 4.9173e-04\n",
      "Epoch 19/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0037 - val_loss: 3.2807e-05\n",
      "Epoch 20/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0036 - val_loss: 7.4715e-06\n",
      "Epoch 21/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0028 - val_loss: 3.3407e-06\n",
      "Epoch 22/100\n",
      "12145/12145 [==============================] - 7s 559us/step - loss: 0.0035 - val_loss: 2.3009e-06\n",
      "Epoch 23/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0034 - val_loss: 2.2605e-06\n",
      "Epoch 24/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0028 - val_loss: 2.5125e-06\n",
      "Epoch 25/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0035 - val_loss: 1.8117e-06\n",
      "Epoch 26/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0075 - val_loss: 0.0080\n",
      "Epoch 27/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0081 - val_loss: 0.0308\n",
      "Epoch 28/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 6.0005e-04 - val_loss: 7.6994e-07\n",
      "Epoch 29/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0087 - val_loss: 0.0031\n",
      "Epoch 30/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.0804e-04 - val_loss: 6.5816e-07\n",
      "Epoch 31/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0031 - val_loss: 6.5566e-07\n",
      "Epoch 32/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0036 - val_loss: 6.8945e-07\n",
      "Epoch 33/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0086 - val_loss: 0.0030\n",
      "Epoch 34/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0014 - val_loss: 5.8741e-07\n",
      "Epoch 35/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0032 - val_loss: 1.1375e-06\n",
      "Epoch 36/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 37/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0039 - val_loss: 8.1577e-07\n",
      "Epoch 38/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 0.0028 - val_loss: 3.3060e-06\n",
      "Epoch 39/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 7.5083e-07 - val_loss: 2.6182e-07\n",
      "Epoch 40/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 2.1919e-07 - val_loss: 1.9083e-07\n",
      "Epoch 41/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.7489e-07 - val_loss: 1.6238e-07\n",
      "Epoch 42/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.5362e-07 - val_loss: 1.4658e-07\n",
      "Epoch 43/100\n",
      "12145/12145 [==============================] - 7s 556us/step - loss: 1.4065e-07 - val_loss: 1.3557e-07\n",
      "Epoch 44/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.3132e-07 - val_loss: 1.2775e-07\n",
      "Epoch 45/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.2449e-07 - val_loss: 1.2165e-07\n",
      "Epoch 46/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.1919e-07 - val_loss: 1.1702e-07\n",
      "Epoch 47/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.1491e-07 - val_loss: 1.1323e-07\n",
      "Epoch 48/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.1150e-07 - val_loss: 1.0999e-07\n",
      "Epoch 49/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.0861e-07 - val_loss: 1.0743e-07\n",
      "Epoch 50/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.0613e-07 - val_loss: 1.0508e-07\n",
      "Epoch 51/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.0412e-07 - val_loss: 1.0318e-07\n",
      "Epoch 52/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 1.0227e-07 - val_loss: 1.0126e-07\n",
      "Epoch 53/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 1.0045e-07 - val_loss: 9.9683e-08\n",
      "Epoch 54/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.9113e-08 - val_loss: 9.8473e-08\n",
      "Epoch 55/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.7965e-08 - val_loss: 9.7405e-08\n",
      "Epoch 56/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.6928e-08 - val_loss: 9.6412e-08\n",
      "Epoch 57/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.5862e-08 - val_loss: 9.5411e-08\n",
      "Epoch 58/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.4934e-08 - val_loss: 9.4565e-08\n",
      "Epoch 59/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.4129e-08 - val_loss: 9.3784e-08\n",
      "Epoch 60/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.3407e-08 - val_loss: 9.3153e-08\n",
      "Epoch 61/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 9.2767e-08 - val_loss: 9.2527e-08\n",
      "Epoch 62/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.2244e-08 - val_loss: 9.2022e-08\n",
      "Epoch 63/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.1774e-08 - val_loss: 9.1535e-08\n",
      "Epoch 64/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.1294e-08 - val_loss: 9.1035e-08\n",
      "Epoch 65/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.0798e-08 - val_loss: 9.0571e-08\n",
      "Epoch 66/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 9.0376e-08 - val_loss: 9.0156e-08\n",
      "Epoch 67/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.9990e-08 - val_loss: 8.9768e-08\n",
      "Epoch 68/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.9624e-08 - val_loss: 8.9379e-08\n",
      "Epoch 69/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.9265e-08 - val_loss: 8.9082e-08\n",
      "Epoch 70/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.8982e-08 - val_loss: 8.8803e-08\n",
      "Epoch 71/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 8.8736e-08 - val_loss: 8.8569e-08\n",
      "Epoch 72/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 8.8501e-08 - val_loss: 8.8311e-08\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.8243e-08 - val_loss: 8.8052e-08\n",
      "Epoch 74/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.7990e-08 - val_loss: 8.7834e-08\n",
      "Epoch 75/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 8.7795e-08 - val_loss: 8.7643e-08\n",
      "Epoch 76/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.7632e-08 - val_loss: 8.7489e-08\n",
      "Epoch 77/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.7459e-08 - val_loss: 8.7323e-08\n",
      "Epoch 78/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.7284e-08 - val_loss: 8.7151e-08\n",
      "Epoch 79/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.7132e-08 - val_loss: 8.7021e-08\n",
      "Epoch 80/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6996e-08 - val_loss: 8.6878e-08\n",
      "Epoch 81/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6854e-08 - val_loss: 8.6721e-08\n",
      "Epoch 82/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 8.6698e-08 - val_loss: 8.6565e-08\n",
      "Epoch 83/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6562e-08 - val_loss: 8.6449e-08\n",
      "Epoch 84/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6411e-08 - val_loss: 8.6289e-08\n",
      "Epoch 85/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6287e-08 - val_loss: 8.6181e-08\n",
      "Epoch 86/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6194e-08 - val_loss: 8.6083e-08\n",
      "Epoch 87/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6113e-08 - val_loss: 8.6000e-08\n",
      "Epoch 88/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.6027e-08 - val_loss: 8.5929e-08\n",
      "Epoch 89/100\n",
      "12145/12145 [==============================] - 7s 558us/step - loss: 8.5967e-08 - val_loss: 8.5862e-08\n",
      "Epoch 90/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5908e-08 - val_loss: 8.5810e-08\n",
      "Epoch 91/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5859e-08 - val_loss: 8.5757e-08\n",
      "Epoch 92/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5784e-08 - val_loss: 8.5693e-08\n",
      "Epoch 93/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5724e-08 - val_loss: 8.5637e-08\n",
      "Epoch 94/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5680e-08 - val_loss: 8.5586e-08\n",
      "Epoch 95/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5631e-08 - val_loss: 8.5555e-08\n",
      "Epoch 96/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5589e-08 - val_loss: 8.5507e-08\n",
      "Epoch 97/100\n",
      "12145/12145 [==============================] - 7s 556us/step - loss: 8.5545e-08 - val_loss: 8.5452e-08\n",
      "Epoch 98/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5497e-08 - val_loss: 8.5401e-08\n",
      "Epoch 99/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5453e-08 - val_loss: 8.5359e-08\n",
      "Epoch 100/100\n",
      "12145/12145 [==============================] - 7s 557us/step - loss: 8.5399e-08 - val_loss: 8.5310e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/anaconda3/envs/my_1.8/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('model_save/aion_slu_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('aion_slu_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                     [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == \"\\n\" or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 감정 표현 동작 하자.\n",
      "\n",
      "Decoded sentence: motion(감정 표현).\n",
      "-\n",
      "Input sentence: 에레슈키갈의 사도 변신를 시작 해줘.\n",
      "\n",
      "Decoded sentence: transform(에레슈키갈의 사도).\n",
      "-\n",
      "Input sentence: 정신 큐빅를 시작 하소서.\n",
      "\n",
      "Decoded sentence: cubic(정신).\n",
      "-\n",
      "Input sentence: 황금 깡통 변신를 하게나.\n",
      "\n",
      "Decoded sentence: transform(황금 깡통).\n",
      "-\n",
      "Input sentence: 최근 퀘스트 빨리 하지.\n",
      "\n",
      "Decoded sentence: quest(최근).\n",
      "-\n",
      "Input sentence: 카이시넬 변신를 빨리 합시다.\n",
      "\n",
      "Decoded sentence: transform(카이시넬).\n",
      "-\n",
      "Input sentence: 감정 표현 동작 하여라.\n",
      "\n",
      "Decoded sentence: motion(감정 표현).\n",
      "-\n",
      "Input sentence: 루나 파견대 변신를 빨리 하게.\n",
      "\n",
      "Decoded sentence: transform(루나 파견대).\n",
      "-\n",
      "Input sentence: 광대 변신를 빨리 하여라.\n",
      "\n",
      "Decoded sentence: transform(광대).\n",
      "-\n",
      "Input sentence: 징표 동작 지금 좀.\n",
      "\n",
      "Decoded sentence: motion(징표).\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 레파르 혁명단 변신 시작 하렴.\n",
      "\n",
      "Decoded sentence: transform(레파르 혁명단).\n",
      "-\n",
      "Input sentence: 레드 펭귄 변신를 지금 하십시오.\n",
      "\n",
      "Decoded sentence: transform(레드 펭귄).\n"
     ]
    }
   ],
   "source": [
    "test_input_texts = []\n",
    "\n",
    "with open(\"data/aion_test_v.txt\", \"r\", encoding='utf=8') as f:\n",
    "    reader =f.readlines()\n",
    "    for i in reader:\n",
    "        test_input_texts.append(i)\n",
    "\n",
    "test_encoder_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "for i, test_input_text in enumerate(test_input_texts):\n",
    "    for t, char in enumerate(test_input_text):\n",
    "        test_encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "for seq_index in range(28, 30):\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_1.8]",
   "language": "python",
   "name": "conda-env-my_1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
